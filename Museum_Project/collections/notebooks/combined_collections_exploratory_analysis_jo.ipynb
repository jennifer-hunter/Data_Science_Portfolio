{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the date columns of the combined collections data\n",
    "\n",
    "The dataset had been merged and data cleansed previously but some outliers had been identified and filtered in visualisations.  This workbook was to explore if there were alternative options to just filtering to clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# This jupyter notebook is to explore the combined colections dataset after the cleaning of Item Dates and Item Place.\n",
    "\n",
    "df = pd.read_csv('../data/combined_collections_dataset.csv', sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick look at our data\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Quick check of the size of the combined dataset\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# check labels for each column\n",
    "\n",
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Check on data types and number of nulls of the columns\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# SUMMARY STATISTICS\n",
    "\n",
    "\"\"\"\n",
    "Key Observations\n",
    "\n",
    "1. Minumum values for StartDate, EndDate and MidpointDate are close to -60,000.  These need to be checked as -6000 would be expected.\n",
    "2. Maximum values for StartDate, EndDate and MidpointDate are close to 193,000.  These need to be checked as 2025 would be expected\n",
    "\n",
    "CONCLUSION: observations 1 and 2 suggests that there are extreme *values Outliers* in our dataset!\n",
    "\n",
    "These columns are complete with no NaN.\n",
    " 0   RecordID       8273 non-null   int64  \n",
    " 1   Museum         8273 non-null   object - this could be useful to compare BM and V&A separately and then together.\n",
    " 2   LocalID        8273 non-null   object - this is not necessary for correlations checks as duplicated in Record ID/\n",
    "\"\"\"\n",
    "\n",
    "df.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "StartDate column\n",
    "\n",
    "1. StartDate are checked to see if there are a range of values in the extremes.\n",
    "\n",
    "\"\"\"\n",
    "plt.scatter(df['StartDate'],df['EndDate'])\n",
    "plt.title('Start Date versus End Date')\n",
    "# First inspect all values where StartDate is > 2025\n",
    "\n",
    "futureItem = (df['StartDate'] > 2025) | (df['EndDate'] > 2025)\n",
    "\n",
    "# Some prints were used to summarise the outlier values\n",
    "\n",
    "# start_outliers_df = df[['RecordID', 'ItemDate', 'StartDate', 'EndDate', 'MidpointDate']][futureItem]\n",
    "# df.StartDate.unique()\n",
    "# start_outliers_summary = pd.Series.value_counts(start_outliers_df.StartDate)\n",
    "# print(start_outliers_df)\n",
    "# print(start_outliers_summary)\n",
    "\n",
    "# RecordIDs with Start Dates in future 5465, 7266, 7884\n",
    "\n",
    "# Next looking for items made before 6000BC which is an outlier.\n",
    "\n",
    "extreme_early = (df['StartDate'] < -6000) | (df['EndDate'] < -6000)\n",
    "\n",
    "# extreme_outliers_df = df[['RecordID', 'ItemDate', 'StartDate', 'EndDate', 'MidpointDate']][extreme_early]\n",
    "# extreme_outliers_summary = pd.Series.value_counts(extreme_outliers_df.StartDate)\n",
    "# print(extreme_outliers_df)\n",
    "# print(extreme_outliers_summary)\n",
    "\n",
    "# RecordID                                      ItemDate  StartDate  \\\n",
    "# 552        553                                     206BC-220   -20600.0   \n",
    "# 810        811                                       80BC-50    -8000.0   \n",
    "# 1033      1034                                     400BC-300   -40000.0   \n",
    "# 1387      1388                 400BC-AD400 (Marpole Culture)   -40000.0   \n",
    "# 1756      1757                                     206BC-220   -20600.0   \n",
    "# 1965      1966                               206 BC - 220 AD   -20600.0   \n",
    "# 2298      2299                             100BC-100 (circa)   -10000.0   \n",
    "# 2450      2451                       580 BC \u2013 550 BC (circa)   -58000.0   \n",
    "# 3003      3004                                     100BC-100   -10000.0   \n",
    "# 3110      3111                                     100BC-100   -10000.0   \n",
    "# 3179      3180                              250 BC -- 130 BC   -25000.0   \n",
    "# 3594      3595                                 100 BC-100 AD   -10000.0   \n",
    "# 3882      3883                                 200BC-100 (?)   -20000.0   \n",
    "# 6537      6538                                   100BC-AD100   -10000.0   \n",
    "# 6669      6670                                ca. 200-100 BC   -10000.0   \n",
    "# 7045      7046  c.180 BC-50 BC or 4th century-5th century AD   -18000.0   \n",
    "# 7825      7826                                206 BC- 220 AD   -20600.0   \n",
    "# 7958      7959                                   206 BC-8 AD   -20600.0\n",
    "\n",
    "# On inspection it seems that in most cases the StartDate, EndDate and MidpointDate could be corrected by dividing by 100\n",
    "\n",
    "df['CorrectedStartDate'] = df['StartDate'][futureItem | extreme_early] / 100\n",
    "df['CorrectedEndDate'] = df['EndDate'][futureItem | extreme_early] / 100\n",
    "\n",
    "# The graph below highlights the extreme values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.CorrectedStartDate.fillna(df.StartDate, inplace=True)\n",
    "df.CorrectedEndDate.fillna(df.EndDate, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series.value_counts(df['CorrectedStartDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['CorrectedStartDate'],df['CorrectedEndDate'])\n",
    "plt.title('Start Date versus End Date Cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# checking for missing (NaN) values with the help of visualization\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "EXPLANATION\n",
    "\n",
    "Dataset has no missing values for:  \n",
    " 0   RecordID       8273 non-null   int64  \n",
    " 1   Museum         8273 non-null   object \n",
    " 2   LocalID        8273 non-null   object \n",
    "\n",
    "Where there is missing data this can be seen in white.  As they are 2 merged datasets the different spread of missing data can be seen here.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "sns.heatmap(df.isnull(),cbar=True,yticklabels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# CHECKING FOR OUTLIERS\n",
    "\"\"\"\n",
    "OBSERVATION\n",
    "\n",
    "There are still outlier values earlier than 3500BC but this is representitive of the data and has been left.\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "import math\n",
    "    \n",
    "plt.figure(1)\n",
    "sns.boxplot(df['CorrectedStartDate'], color='black', orient='v')\n",
    "plt.figure(2)\n",
    "sns.boxplot(df['CorrectedEndDate'], color='black', orient='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# check distribution-skewness\n",
    "\n",
    "plt.figure(1)\n",
    "sns.displot(df['CorrectedStartDate'],kde=True)\n",
    "plt.figure(2)\n",
    "sns.displot(df['CorrectedEndDate'],kde=True)\n",
    "\n",
    "# There are 2 peaks highlighting increased collections of items at these production dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['CorrectedStartDate'].describe())\n",
    "# inner quartitle range between 380BC and 1729AD.  Median is 1206AD.\n",
    "print(f' Mean: {df['CorrectedStartDate'].mean()}\\n Median: {df['CorrectedStartDate'].median()}\\n Mode: {df['CorrectedStartDate'].mode()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CorrectedMidpointDate'] = round((df['CorrectedStartDate']+((df['CorrectedEndDate']-df['CorrectedStartDate'])/2)),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CorrectedMidpointDate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MidpointDate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing the AcqDate Column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From info() we found that there were only 7822 non-null values of type object.  These should be dates (years) so we will try to convert to numeric. \n",
    "df['AcqDate'] = pd.to_numeric(df['AcqDate'], errors='coerce')\n",
    "# I will not include year 0AD in my selection so NaN values can be converted to 0 to support int conversion.\n",
    "df['AcqDate'] = df['AcqDate'].replace('NaN', pd.NA).fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AcqDate'].nunique()\n",
    "\n",
    "# 227 uniques values available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the records with no dates\n",
    "condition = df['AcqDate'] > 0\n",
    "Acq_only_df = df['AcqDate'][condition]\n",
    "\n",
    "Acq_only_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Acq_count_df = df['AcqDate'].value_counts().reset_index().sort_values(by='AcqDate', ascending=True).reset_index()\n",
    "Acq_df_sort = Acq_count_df.sort_values(by='count', ascending=False).reset_index()\n",
    "Acq_df_sort.drop(index=0, inplace=True)\n",
    "Acq_df_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Acq_only_df.plot.hist(bins=10)\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Acquisitions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This histogram highlights is a good visualisation of the small percentage of items acquired after 2000. This will make analysis open to bias and results would not be suitable for modelling or predictions.\n",
    "\n",
    "Calculation of percentage below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = df['AcqDate'] > 2004\n",
    "condition2 = df['AcqDate'] < 2025\n",
    "footfall_df = df['AcqDate'][condition & condition2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Percentage of Acquistions in range of footfall dates = ({len(footfall_df)} / {len(Acq_only_df)} * 100) = {round((len(footfall_df)/len(Acq_only_df))*100, 2)}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Footfall_acq_counts = footfall_df.value_counts().sort_index(ascending=True).reset_index()\n",
    "values = Footfall_acq_counts['count']\n",
    "x_label = Footfall_acq_counts['AcqDate'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values could be used to compare against visitor footfall in the same year but consideration would be needed to identify the delay from acquisition to any impact to the visitor numbers.  As items are not necessarily on public display it was felt that this line of analysis would not be of value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Footfall_acq_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a combined_collection_for_visualisations\n",
    "\n",
    "To include this cleansed data into visualisations it would be useful to have the corrected columns in a new csv file.\n",
    "\n",
    "I plan to filter the information to include only items that have an acquisition date between 2005 - 2024 to match with footfall data available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df= df[['RecordID', 'Museum', 'AcqDate', 'ObjectType', 'ItemDate', 'ModernCountry',\n",
    "        'ItemMaterial', 'ItemTechnique', 'CorrectedStartDate',\n",
    "       'CorrectedEndDate', 'CorrectedMidpointDate']][condition & condition2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_csv('../data/combined_collections_footfall_dates_dataset.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}